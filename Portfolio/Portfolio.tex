
\documentclass[sigconf]{acmart}



\begin{document}

\title{Compilers Portfolio}

\author{Ben Barnett}
\affiliation{%
  \institution{Montana State University}
}
\email{benbarn313@gmail.com}

\author{Cooper Strahan}
\affiliation{%
  \institution{Montana State University}
}
\email{cooperstrahan@gmail.com}

\begin{teaserfigure}
  \includegraphics[width=\textwidth]{msu}
  \caption{Montana State University.}
  \label{fig:teaser}
\end{teaserfigure}

\maketitle

\section{Program}
\section{Teamwork}
\section{Design Pattern}
\section{Technical Writing}

\subsection{Introduction}

\subsection{Background}

\subsection{Methods and Discussion}

\subsubsection{Tools and Setup} 
The first main step in this project is to build a scanners. Scanners are used in the compiler for any given programming language. Say you want to write your own programming language; a scanner would form the basic rules for your computer to be able to interpret your language. Effectively, the function of a scanner is the first step in translating code into machine language. In this step we installed ANTLR to our individual machines. ANTLR is a processing language that allows users to create files consisting of regular expressions that will translate text. This language parsing tool will allow us to build our own 'Toy' programming language, but can be used to create larger scale languages. After installing ANTLR we added its' paths to our environment variables and created aliases for consistent use in the command line. Once this was set up and we tested the basic usage of ANTLR by defining the Hello.g4 grammar described on ANTLR's github setup page. We then created a github repository for our project files. We both installed IntelliJ because of its great Version Control capabilities, we plan on developing our project with Java.
\newpage
\subsubsection{Scanner} 
In this step we gave a set of token definitions to the ANTLR scanner generation tool. We were then able to tokenize various programs in the LITTLE language. The whole point of tokenization is to break down code into smaller parts, or tokens, that are usually comprised of a few characters. The point of this is to organize the code into small parts that are recognized by the parser(the next step in compilation).
 Programmers then use regular expressions, a sequence of characters that define a search pattern, to match words, symbols, and numbers to their given meanings. These meanings are separated into five different categories; Identifiers, Keywords, Separators, Literals, and Comments. Identifiers are names given by programmers to store data, Keywords are words reserved by the programming language that cause the program to exhibit a specific behavior, Separators are used by programmers to group and split up the other tokens, Operators are used to perform mathematical functionality, Literals are numbers or words many times being stored by identifiers, and Comments are phrases written by the programmer to others, or their future self, that are intended to explain the program.
\newline
Guided by this theory of scanners, we developed a Scanner for the LITTLE programming language. The first thing we did for our scanner was write a grammar file. This file is comprised of regular expressions that assign categories to the input text. We then used the ANTLR tool to generate a Lexer, Listener, and Parser. These programs allow us to go far beyond just tokenizing the code, but we have yet to explore all of their functionality. In building our scanner, we first chose to use the Java programming language. This decision was based on many factors, the first being the flawless integration with the ANTLR tool, which significantly aided us implementing our scanner. Furthermore, Java is both my partner and I's strongest programming language, which allowed us to work quickly and effectively and trouble-shoot any problems we ran into. Our next method to insure success was to include the input and ANTLR.jar files within our project so that they would be locally accessible at run time, which allows our project to be portable across multiple development environments. Another deliberate decision we made was to implement many of the objects suggested by our textbook, The Definitive ANTLR 4 Reference, which helped us streamline the process of reading input files without having to dive into the documentation. We also chose to use a string array to more easily read through the input text files, and then used a loop to iterate through the text files in order to generate our desired output files. Ultimately, our methods and design choices allowed us to create a concise and efficient program that accomplishes our desired goal.
\newline
The first and most significant challenge we ran into was, being very new to the ANTLR library, implementing the functionality given to us by ANTLR, which meant we had to expend extra efforts to insure our understanding. The first programming challenge we encountered was understanding how to read in the token stream, which allows us access to the tokenized version of our input file. By reading through the documentation we discovered that the token stream's consume method gave us the ability to read through the input file. Our next issue arose when we ran the consume method, because the method auto-increments the index of our token stream, we could either get the first or the last index of the stream but not both. In order to get around this issue we had to write a clause within our loop that allowed us to jump out of the loop right before we caused an error by trying to run the getType method on the End Of Function literal. Overall, reading the documentation was extremely helpful in helping us solve all of the issues that we ran into while we were writing our scanner.
\newpage
\subsubsection{Parser} 
\newline
Parsing is the analysis of language based on a defined set of rules. In order for any language to make sense be it natural or computer, we must define the language within a certain scope. For instance in the English language all sentences, in order to be grammatically correct, must contain a subject and a predicate. The letters and the words that we defined with our Lexer are now being forced into specific structures such that we can make sense of them. For many people in the United States, we may not have thought of these rules until they were introduced to us in Middle and High School. Rather than being explicitly taught them in school, we learned these rules as we learned speech, some of us maybe slightly better than others (myself not included). As you think more about parsing or syntactic analysis you have to think more and more about the rules that define any language. One of the reasons that people have such a difficult time learning languages other than their natural born language, is because the memorization of words built with a slightly different structure than their own. This fact combined with the presentation of new(ish) grammar rules, which they may have never learned in the first place, to define these new words, can make new learners feel completely overwhelmed. One of the reasons that people who cannot program, find programming such a difficult idea to wrap their head around, is just this! They are challenged by new syntax and the new rules that govern this syntax. To the advantage of the programmer who can already write code, many programming languages are written with similar syntax and the languages are defined with relatively similar rules. One of the reasons for this is because for many of the most popular programming languages, the languages are all themselves written in C. This may lead a person to believe that learning to program in C would then give them a terrific grasp of programming in general, and they would be correct.
\newline
Continuing with this idea of trying to learn languages, in the case of computer programming, many of these programming languages are not just static things. Every day programmers are writing code to fix problems within already written languages, or even writing completely new programming languages. The way that these languages are being defined are by Context Free Grammars. Context Free Grammars are the set of rules that a programming language must conform to. Relative to the English language where every sentence must contain a subject and a predicate, each program written in a specific language must conform to the rules of that language. If the program does not conform to the rules of the language, the program will not be able to execute properly, thus rendering the program ineffective. That being said, just as with natural language, programs still have quite a bit of flexibility in what you can write. Programs also are very powerful things, and are able to accomplish very efficiently things that people trying to do by hand would take decades and even millennia to actually complete. This means that the rules themselves for our language, although they need to be fairly strict, must have a certain degree of flexibility in what they accept from the programmer. This forces our Grammar to contain rules that have a recursive nature. Recursion itself being something that could be called on itself until a certain clause causes the thing to return a value to itself, repeatedly until it returns to the first time that it was called. 
\newline 
Context Free Grammars are composed of four things, A finite set of terminals, a finite set of non-terminals, a start symbol, and a finite set of productions. A production is a rewrite rule that is composed of terminals and non-terminals. A terminal is any concrete value that would appear in the program, a non-terminal is another production rule given a symbol as a placeholder. The non-terminals allow someone writing a grammar to recursively create more productions in order to match an input string. The terminals and non terminals together make up the vocabulary of the grammar.
\newline
\begin{figure}
  \includegraphics[width=5cm]{Images/Screen Shot 2020-02-27 at 5.09.19 PM.png}
  \caption{An example Context Free Grammar}
\end{figure}
\newline
There are two main ways to create derivations of a language. Starting at the start state, productions may have multiple different non-terminals that you could potentially choose to expand. One way to approach this would be by expanding the left most non terminal symbol, until the production only consists of terminal values. If you choose to expand your grammar this way you would create the left most derivation of the language. This left most derivation is mainly used in conjunction with a top down parser which is discussed later. The other option is by starting at the right most non terminal and expanding non-terminals from right to left. This method may seem a little bit counter intuitive but is the derivation used in bottom up parsing.
\newline
\begin{figure}
  \includegraphics[width=5cm]{Images/Screen Shot 2020-02-27 at 5.09.26 PM.png} 
  \caption{An example of a Left Most Derivation}
\end{figure}
\newline
\newline
In many instances programmers use Parsers to create a Parse Tree. These Parse Trees give structure to arbitrary programs that fit within the rules of the grammar. The root node of the tree is the start symbol, and the nodes on these trees are made up of the vocabulary of the grammar as well as the $\lambda$ symbol which just denotes an empty string. If we are able to create a parse tree with no errors we can determine that the string input (a program) is a member of our grammar's language. 
\newline
\begin{figure}
  \includegraphics[width=5cm]{Images/Screen Shot 2020-02-27 at 5.09.38 PM.png}
  \caption{An example Parse Tree}
\end{figure}
\newline
Although Context Free Grammars are incredibly powerful tools, they cannot do everything we need. There are certain instances that rules cannot be written for. In the instance of requiring a variable to be declared before it is used, we cannot use a Context Free Grammar to define this rule and must look elsewhere to create similar rules. Along with this Context Free Grammars can contain errors similar to how programs contain errors. We could potentially have unreachable non-terminals which would be deemed useless. If our grammar does not contain any useless non-terminals it would be said to be reduced. Grammars could also be ambiguous. An ambiguous grammar is one that would allow us to create multiple different parse trees for the same symbol. In general we only want to use unambiguous grammars. Our last frequent error is that we could have written a rule incorrectly causing our grammar to generate an incorrect language. This is somewhat ironic because our grammar is supposed to be the definition of our language. We could make sure that this does not happen by passing in a language that we know to exist within our grammar and making sure that the parse tree looks the way we want it to look.
\newline
If we are certain that our grammar is unambiguous. We can create an algorithm that reads in an input, determines that the input is valid and is output as a parse tree data structure we can say that our algorithm is a parser. In general there are two approaches to parsing, as mentioned earlier, they are top down and bottom up parsing. A parser would be a top down parser if it started at the start symbol, the top of the parse tree, and realizes each node in the parse tree data structure by expanding its non-terminals. The non-terminals are expanded in a predictive manner because they "predict" the next production before matching the input string that corresponds to the production. On the other hand bottom up parsing starts at the bottom of our parse tree and then uses productions to match parent nodes, effectively iterating up the tree back to the start state.
\newline
The productions predicted by a top down parse correspond to the left most derivation. The productions seen by a bottom up parse correspond to a right most derivation. These govern the most widely used parsing techniques which are LL and LR. An LL parser reads in a string from left to right to create the left most derivation (\underline{L}eft to Right, \underline{L}eft most Derivation). An LR parser reads in a string from left to right and creates the right most derivation (\underline{L}eft to Right, \underline{R}ight Most Derivation).
\newline
These decisions are easy for a person to see and make especially with a small language, and short grammar. For a computer these decisions are not so easy. The computer builds first and follow sets. Which are sets of terminals that can be accessed when a production is used. The computer uses the first and follow sets to create a table that determines what production the computer will "predict" dependent on the current state of the parse "stack" and the token being read in. 
\newline
 In our project we created an LL(1) parser (The one signifies that it only looks at the next incoming token). Our parser was written for a language called LITTLE. The Context Free Grammar we used was essentially given to us. The only edits to the given Context Free Grammar were placing '' around terminal values. We then adjusted the Driver from the Lexer so that it would parse files and determine if a given file was in the LITTLE language. We had to turn off error listeners to make sure that we weren't sending bad output to the console. Other than that we just identified whether or not a given file was in the language, if it was we printed Accepted if it was not we printed Not accepted. We determined if the file was in our language by counting the number of errors reported. If there were any errors we did not allow the file in our language. 
\newline
  

\subsubsection{Symbol Table} 
\subsubsection{Code Generation} 
\subsubsection{Full-fledged compiler}

\subsection{Conclusion and Future Work}

\section{UML}
\section{Design Trade-offs}
\section{Software development life cycle model}

\section{Image credit}
https://courses.cs.washington.edu/courses/csep521/99sp/lectures/lecture13/img028.JPG
\end{document}
\endinput

